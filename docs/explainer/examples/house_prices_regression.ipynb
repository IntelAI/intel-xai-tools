{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Explaining house prices\n",
    "\n",
    "\n",
    "See [Interpreting text models: IMDB sentiment analysis](https://captum.ai/tutorials/House_Prices_Regression_Interpret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to apply `Captum` library on a regression model and understand important features, layers / neurons that contribute to the prediction. It compares a number of attribution algorithms from `Captum` library for a simple DNN model trained on a sub-sample of a well-known Boston house prices dataset.\n",
    "\n",
    "Note that in order to be able to run this notebook successfully you need to install scikit-learn package in advance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# adds house_prices_regression.pkz to sys.path which includes a captum entry_point. See house_prices_regression.yaml\n",
    "# to create the house_prices_regression.pkz run 'explainer export house_prices_regression'\n",
    "from explainer.explainers import house_prices_regression\n",
    "\n",
    "# scikit-learn related imports\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# pytorch relates imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load boston house prices dataset and corresponding labels from scikit-learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "# feature_names -> ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "feature_names = boston.feature_names\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retain deterministic results, let's fix the seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 70% of our data for training and the remaining 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize dependent variable vs each independent variable in a separate plot. Apart from that we will also perform a simple regression analysis and plot the fitted line in dashed, red color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 3, ncols=5, figsize=(30, 20))\n",
    "for i, (ax, col) in enumerate(zip(axs.flat, feature_names)):\n",
    "    x = X[:,i]\n",
    "    pf = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(pf)\n",
    "\n",
    "    ax.plot(x, y, 'o')\n",
    "    ax.plot(x, p(x),\"r--\")\n",
    "\n",
    "    ax.set_title(col + ' vs Prices')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Prices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram above we can tell that some of the most influential features that are correlated with the output price are:   \n",
    "   - RM, the average numbers of rooms in the houses of the neighborhood.\n",
    "     If RM increases the house price increases too.\n",
    "   - LSTAT, the percentage of the house-owners in the neighborhood (lower class).\n",
    "     This variable is negatively correlated with the price. The lower the class the less likely is that the person will be able to afford an expensive house.\n",
    "   - PTRATIO, the pupil-teacher ratio by town.\n",
    "     If the pupil-teacher ratio increases then the house price decreases, resulting in a negative correlation between ptratio and house price.\n",
    "     \n",
    "These features are being identified as important also from others and can be found by an online search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorizing inputs and creating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we tensorize input features and corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).view(-1, 1).float()\n",
    "\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).view(-1, 1).float()\n",
    "\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining default hyper parameters for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "size_hidden1 = 100\n",
    "size_hidden2 = 50\n",
    "size_hidden3 = 10\n",
    "size_hidden4 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a four layer neural network containing ReLUs between each linear layer. This network is slightly more complex than the standard linear regression model and results in a slightly better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(13, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.lin4 = nn.Linear(size_hidden3, size_hidden4)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin4(self.relu3(self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input)))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BostonModel()\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Boston Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function that will be used for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the training function that contains the training loop and uses RMSprop and given input hyper-parameters to train the model defined in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_inp, num_epochs = num_epochs):\n",
    "    optimizer = torch.optim.RMSprop(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_iter:\n",
    "            # forward pass\n",
    "            outputs = model_inp(inputs)\n",
    "            # defining loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # computing gradients\n",
    "            loss.backward()\n",
    "            # accumulating running loss\n",
    "            running_loss += loss.item()\n",
    "            # updated weights based on computed gradients\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:    \n",
    "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                  (epoch + 1, num_epochs, running_loss))\n",
    "        running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model was previously trained and stored, we load that pre-trained model, otherwise, we train a new model and store it for future uses.\n",
    "\n",
    "Models can found here: https://github.com/pytorch/captum/tree/master/tutorials/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_load_save_model(model_obj, model_path):\n",
    "    if path.isfile(model_path):\n",
    "        # load model\n",
    "        print('Loading pre-trained model from: {}'.format(model_path))\n",
    "        model_obj.load_state_dict(torch.load(model_path))\n",
    "    else:    \n",
    "        # train model\n",
    "        train(model_obj)\n",
    "        print('Finished training the model. Saving the model to the path: {}'.format(model_path))\n",
    "        torch.save(model_obj.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = '../../models/boston_model.pt'\n",
    "train_load_save_model(model, SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a simple sanity check and compute the performance of the model using Root Squared Mean Error (RSME) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "\n",
    "print('model err: ', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different attribution algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the attributions with respect to the inputs of the model using different attribution algorithms from core `Captum` library and visualize those attributions. We use test dataset defined in the cells above for this purpose.\n",
    "\n",
    "We use mainly default settings, such as default baselines, number of steps etc., for all algorithms, however you are welcome to play with the settings. For GradientSHAP specifically we use the entire training dataset as the distribution of baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize attribution scores with respect to inputs (using test dataset) for our simple model in one plot. This will help us to understand how similar or different the attribution scores assigned from different attribution algorithms are. Apart from that we will also compare attribution scores with the learned model weights.\n",
    "\n",
    "It is important to note the we aggregate the attributions across the entire test dataset in order to retain a global view of feature importance. This, however, is not ideal since the attributions can cancel out each other when we aggregate then across multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entry_point\n",
    "entry_point.entry_point(model, feature_names, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitudes of learned model weights tell us about the correlations between the dependent variable `Price` and each independent variable. Zero weight means no correlation whereas positive weights indicate positive correlations and negatives the opposite. Since the network has more than one layer these weights might not be directly correlated with the price.\n",
    "\n",
    "From the plot above we can see that attribution algorithms sometimes disagree on assigning importance scores and that they are not always aligned with weights. However, we can still observe that the top important three features: `LSTAT`, `RM` and `PTRATIO` are also considered to be important based on both most attribution algorithms and the weight scores.\n",
    "\n",
    "It is interesting to observe that the feature `B` has high positive attribution score based on some of the attribution algorithms. This can be related, for example, to the choice of the baseline. In this tutorial we use zero-valued baselines for all features, however if we were to choose those values more carefully for each feature the picture will change. Similar arguments apply also when the signs of the weights and attributions mismatches or when one algorithm assigns higher or lower attribution scores compare to the others.\n",
    "\n",
    "In terms of least important features, we observe that `CHAS` and `RAD` are voted to be least important both based on most attribution algorithms and learned coefficients.\n",
    "\n",
    "Another interesting observation is that both Integrated Gradients and DeepLift return similar attribution scores across all features. This is associated with the fact that although we have non-linearities in our model, their effects aren't significant and DeepLift is close to `(input - baselines) * gradients`. And because the gradients do not change significantly along the straight line from baseline to input, we observe similar situation with Integrated Gradients as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainable-ai-tooling",
   "language": "python",
   "name": "explainable-ai-tooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f116785276e6c3ad7d08e849fd53d61348b7787ff6ff002b8f1b3c8ab5c4540"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
