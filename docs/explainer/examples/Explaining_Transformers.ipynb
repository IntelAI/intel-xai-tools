{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining transformer models using SHAP\n",
    "## CHAPTER 07 - *Practical exposure of using SHAP in ML*\n",
    "\n",
    "From **Applied Machine Learning Explainability Techniques** by [**Aditya Bhattacharya**](https://www.linkedin.com/in/aditya-bhattacharya-b59155b6/), published by **Packt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The goal of this notebook is to explore model explainability of transformer models trained on text data. Please check out *Chapter 7 - Practical exposure of using SHAP in ML* for other interesting approaches of using SHAP in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following libraries in Google Colab or your local environment, if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy shap transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainer.explainers import lm_zeroshot_explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import shap\n",
    "print(f\"Shap version used: {shap.__version__}\")\n",
    "import transformers\n",
    "print(f\"Hugging Face transformer version used: {transformers.__version__}\")\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP and Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SHAP and HF](https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter07/images/Shap_HF.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will apply SHAP for explaining Hugging Face transformer models trained on text data. Hugging Face Transformers (https://github.com/huggingface/transformers) are state of the art pre-trained transformer models trained on huge amount of data. This framework provides you the option of utilizing pre-trained models on various applications using just few lines of code. The pre-trained models can be easily downloaded and fine-tuned on any custom dataset and can be integrated very easily with other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the hugging face transformer models for text sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining transformer based Sentiment Analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the transformers pipeline model for Sentiment analysis\n",
    "model = transformers.pipeline('sentiment-analysis', return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the text data\n",
    "text_data = \"Hugging face transformers are absolutely brilliant!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(text_data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the model predicted the outcome to be positive with a very high confidence score, which is correct! Now, let's see if this can be explained using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SHAP explainer object\n",
    "explainer = shap.Explainer(model) \n",
    "shap_values = explainer([text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[0,:,'POSITIVE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0,:,'POSITIVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see, using the force plot and bar plots in SHAP, we can visualize the words which contributes positively and the words which contribute negatively towards the model prediction. Now, let's try out with a multi-class classification example using the Hugging Face transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Multi-Class prediction transformer models using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the hugging face model and tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\", use_fast=True)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\n",
    "\n",
    "# build a pipeline object to do predictions\n",
    "pipeline = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explaining using SHAP\n",
    "explainer = shap.Explainer(pipeline)\n",
    "shap_values = explainer([text_data])\n",
    "shap.plots.text(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the feature importance towards the outcome - joy\n",
    "shap.plots.bar(shap_values[0,:,'joy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the feature importance towards the outcome - sadness\n",
    "shap.plots.bar(shap_values[0,:,'sadness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have applied SHAP to explain a pre-trained multi-class classification model from Hugging Face. The pre-trained model is used to detect the emotions - *sadness, joy, love, anger, fear, surprise* from text data. Using SHAP's force plot and bar plots, we can visualize the influence of each word present in the sentence towards the model's prediction. For the selected example - \"*Hugging face transformers are absolutely brilliant!*\", the words *brilliant* and *absolutely* contribute the most towards model's prediction as *joy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the outcome *sadness*, we can see that almost all the words contribute negatively and this signifies that the outcome can never be *sadness*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us try out another very interesting concept used in NLP - Zero-Shot Learning. Zero-Shot Learning enables us to apply a trained model without fine-tuning on any labelled sample. Check out this: https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681 to find out more on using Zero-Shot learning using hugging face transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Zero-Shot Learning using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the hugging face model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"valhalla/distilbart-mnli-12-3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/distilbart-mnli-12-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom pipeline that only requires the text parameter \n",
    "# for the __call__ method and provides a method to set the labels\n",
    "class ZeroShotModelPipeline(ZeroShotClassificationPipeline):\n",
    "    # Overwrite the __call__ method\n",
    "    def __call__(self, *args):\n",
    "        out = super().__call__(args[0], self.set_labels)[0]\n",
    "\n",
    "        return [[{\"label\":x[0], \"score\": x[1]}  for x in zip(out[\"labels\"], out[\"scores\"])]]\n",
    "\n",
    "    def set_labels(self, labels: Union[str,List[str]]):\n",
    "        self.set_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_and_visualize(text, shap_values):\n",
    "    prediction = pipe(text)\n",
    "    print(f\"Model predictions are: {prediction}\")\n",
    "    shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I love playing cricket!\"]\n",
    "labels = [\"insect\",\"sports\", \"animal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the labels for the classification model\n",
    "model.config.label2id.update({v:k for k,v in enumerate(labels)})\n",
    "model.config.id2label.update({k:v for k,v in enumerate(labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ZeroShotModelPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "pipe.set_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explainer\n",
    "explainer = shap.Explainer(pipe)\n",
    "shap_values = explainer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_and_visualize(text, shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the feature importance towards the outcome - sports\n",
    "shap.plots.bar(shap_values[0,:,'sports'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face transformer framework, allows us to apply Zero-Shot learning in just few lines of code using pre-trained models without re-training or fine tuning on any labeled sample. Now, we have applied this model on a slightly difficult example. The example used is \"I love playing cricket!\". *Cricket* can be a sport or even it can an insect. So, ideally, the word cricket individually can contribute towards both the categories. But the term '*playing cricket*' definitely indicates the category sports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model correctly predicted the category as sports, but when we used SHAP, we found out that the word *Cricket* has a negative contribution towards the model prediction. These are situations where model explainability is extremely crucial. Ideally speaking, the most important word that should contribute towards the correct prediction of the class sports, should have been *Cricket* and not *love*. This tells us that model is not robust enough and does have scope of improvement especially for the classes used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are indeed state of the model, especially for text data. Pre-trained transformer does make it easy to productionalize any intelligent system, without the hassle of training the algorithm from scratch. But explaining complicated models like Transformers can be really hard! In this tutorial, we have seen how easily SHAP can be used to explain transformer models from Hugging Face!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transformer Model - https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
    "2. SHAP GitHub Project - https://github.com/slundberg/shap\n",
    "3. SHAP Documentations - https://shap.readthedocs.io/en/latest/index.html\n",
    "4. Hugging Face Transformer - https://github.com/huggingface/transformers\n",
    "5. Hugging Face Zero-Shot Learning - https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681\n",
    "6. Zero-Shot Learning - https://towardsdatascience.com/zero-and-few-shot-learning-c08e145dc4ed\n",
    "7. Some of the utility functions and code are taken from the GitHub Repository of the author - Aditya Bhattacharya https://github.com/adib0073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
