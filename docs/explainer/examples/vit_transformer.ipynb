{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing explainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intel-tensorflow==2.9.1\n",
    "# shap @ git+https://github.com/slundberg/shap@v0.41.0\n",
    "from explainer.explainers import feature_attributions_explainer, metrics_explainer\n",
    "feature_attributions_explainer.entry_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\n",
    "model = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "classes = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "classes[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "datasets_list = list_datasets() \n",
    "pprint(datasets_list,compact=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/dog10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, BasicTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import shap\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128, truncation=True) for v in x])\n",
    "    attention_mask = (tv!=0).type(torch.int64)\n",
    "    logit_arr = model(tv,attention_mask=attention_mask).logits.detach().cpu().numpy()\n",
    "    scores = sp.special.expit(logit_arr)\n",
    "    return scores\n",
    "\n",
    "def custom_tokenizer(text, return_offsets_mapping=True):\n",
    "    text = text.lower()\n",
    "    wordpunct = BasicTokenizer()\n",
    "    splitted_text_list = wordpunct.tokenize(text)\n",
    "    pos_list = []\n",
    "    pos=0\n",
    "    for item in splitted_text_list: \n",
    "      start = text.find(item, pos)\n",
    "      end = start + len(item)\n",
    "      pos_list.append((start, end))\n",
    "      pos = end\n",
    "    out={}\n",
    "    out[\"input_ids\"] = splitted_text_list\n",
    "    if return_offsets_mapping: \n",
    "      out['offset_mapping']=pos_list\n",
    "    return out\n",
    "\n",
    "masker = shap.maskers.Text(custom_tokenizer)\n",
    "masker.mask_token = ''\n",
    "explainer = shap.Explainer(f, masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X[:100])\n",
    "\n",
    "# plot the SHAP values for the positive class\n",
    "shap.plots.beeswarm(shap_values[...,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "\n",
    "from explainer.explainers import metrics_explainer\n",
    "\n",
    "pstats = metrics_explainer['pstats']('explainer = shap.Explainer(f, masker)')\n",
    "pstats.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstats.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pinfo explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "sample_text = 'Crossover comfort food with a redemptive twist'\n",
    "Z=masker.clustering(sample_text)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "dn = dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "# pose sequence as a NLI premise and label (politics) as a hypothesis\n",
    "premise = 'Counterfactuals'\n",
    "hypothesis = 'Machine Learning'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
    "logits = model(input_ids)[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "true_prob = probs[:,1].item() * 100\n",
    "print(f'Probability that the label is true: {true_prob:0.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "model = AutoModel.from_pretrained('deepset/sentence_bert')\n",
    "\n",
    "sentence = 'What explainability methods are most often used with the RoBERTa model?'\n",
    "labels = ['Counterfactual', 'Contrastive', 'GradCAM', 'ScoreCAM',\n",
    "          'LayerCAM', 'Smooth gradient', 'Integrated gradient',\n",
    "          'Partial Dependence Plot', 'Accumulated Local Effects',\n",
    "          'TCAV'\n",
    "         ]\n",
    "\n",
    "# run inputs through model and mean-pool over the sequence\n",
    "# dimension to get sequence-level representations\n",
    "inputs = tokenizer.batch_encode_plus([sentence] + labels,\n",
    "                                     return_tensors='pt',\n",
    "                                     pad_to_max_length=True)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "output = model(input_ids, attention_mask=attention_mask)[0]\n",
    "sentence_rep = output[:1].mean(dim=1)\n",
    "label_reps = output[1:].mean(dim=1)\n",
    "\n",
    "# now find the labels with the highest cosine similarities to\n",
    "# the sentence\n",
    "similarities = F.cosine_similarity(sentence_rep, label_reps)\n",
    "closest = similarities.argsort(descending=True)\n",
    "for ind in closest:\n",
    "    print(f'label: {labels[ind]} \\t similarity: {similarities[ind]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "47de4e6fcdf76d9f7e9823221d58331110ca8a86e4fcaa17b27f269bc08adee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
