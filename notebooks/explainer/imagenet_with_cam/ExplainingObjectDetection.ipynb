{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Object Detection Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The goal of this notebook is to explore various CAM methods for object detection models.\n",
    "Unlike object classification model, we are restricted to use gradient free CAM method since the outputs from object detection model are typically not differentiable.\n",
    "Among the gradient free methods, for now, we support EigenCAM which is an extremely fast method.\n",
    "Let's have a look at two cases: FasterRCNN and YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Notebook Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Intel XAI tools PyTorch CAM Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_ai_safety.explainer.cam import pt_cam as cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Notebook Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the input image\n",
    "Load the input image as a numpy array in RGB order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread(\"https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/both.png\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EigenCAM for FasterRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "Load the trained model depending on how the model was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained=True).eval() # Let's use FasterRCNN ResNet50 FPN as our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the image classification example, choose the target layer (normally the last convolutional layer) to compute CAM for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of FasterRCNN, there is no class name in the output from the model. Thus, to print the name of class with corresponding bounding box in the image, we need to specify the name of classes that are trained as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
    "              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
    "              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
    "              'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.random.uniform(0, 255, size=(len(class_labels), 3)) # Create a different color for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to process the outputs from the object detection model. In the case of FasterRCNN, first, convert the image to a tensor and get the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.float32(image) / 255\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "input_tensor = transform(rgb_img)\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "output = model(input_tensor)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below processes the outputs from the model. We can get the bounding box coordinates, class names, class indices, and box colors of the detected objects with a higher detection score than a threshold value. If you use other object detection models than FasterRCNN, you need to make your own function to match the structure of the outputs from this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output_fasterrcnn(output, class_labels, color, detection_threshold):\n",
    "    boxes, classes, labels, colors = [], [], [], []\n",
    "    box = output['boxes'].tolist()\n",
    "    name = [class_labels[i] for i in output['labels'].detach().numpy()]\n",
    "    label = output['labels'].detach().numpy()\n",
    "\n",
    "    for i in range(len(name)):\n",
    "        score = output['scores'].detach().numpy()[i]\n",
    "        if score < detection_threshold:\n",
    "            continue\n",
    "        boxes.append([int(b) for b in box[i]])\n",
    "        classes.append(name[i])\n",
    "        colors.append(color[label[i]])\n",
    "\n",
    "    return boxes, classes, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.9\n",
    "boxes, classes, colors = process_output_fasterrcnn(output, class_labels, color, detection_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the other important part which differs from the object classification models. We need to write a custom reshape transform to get the activations from the model and process them into 2D format. In the case of FasterRCNN, the backbone outputs 5 different tenors with different spatial size as an Ordered Dict. Thus, we need a custom function which aggregates these image tensors, resizes them to a common shape, and concatenates them. If you use other models than FasterRCNN, you might need to write your own custom reshape transform function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterrcnn_reshape_transform(x):\n",
    "    target_size = x['pool'].size()[-2 : ]\n",
    "    activations = []\n",
    "    for key, value in x.items():\n",
    "        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n",
    "    activations = torch.cat(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "The left image is the computed EigenCAM on the image. On the right side, we can renormalize the EigenCAM inside every bounding box and zero outside the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigencam = cam.eigencam(model, target_layer, boxes, classes, colors, fasterrcnn_reshape_transform, image, 'cpu')\n",
    "eigencam.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EigenCAM for YOLO\n",
    "Computing EigenCAM for YOLO is much simpler than FasterRCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the input image\n",
    "Load the input image as a numpy array in RGB order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/f/f1/Puppies_%284984818141%29.jpg\")\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "Load the trained model depending on how the model was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) # Let's use YOLO5 as our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.model.model.model[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.random.uniform(0, 255, size=(80, 3)) # Create a color list with the length of the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.resize(image, (640, 640))\n",
    "output = model(image)\n",
    "output = output.pandas().xyxy[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below processes the outputs from YOLO model. We already have class names in the output from YOLO, so there is no need to specify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output_yolo(output, color, detection_threshold):\n",
    "    boxes, classes, labels, colors = [], [], [], []\n",
    "\n",
    "    for i in range(len(output[\"xmin\"])):\n",
    "        confidence = output[\"confidence\"][i]\n",
    "        if confidence < detection_threshold:\n",
    "            continue\n",
    "        xmin = int(output[\"xmin\"][i])\n",
    "        ymin = int(output[\"ymin\"][i])\n",
    "        xmax = int(output[\"xmax\"][i])\n",
    "        ymax = int(output[\"ymax\"][i])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        classes.append(output[\"name\"][i])\n",
    "        colors.append(color[int(output[\"class\"][i])])\n",
    "\n",
    "    return boxes, classes, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.4\n",
    "boxes, classes, colors = process_output_yolo(output, color, detection_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of YOLO, we don't need a reshape transform function since we’re getting a 2D spatial tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigencam = cam.eigencam(model, target_layer, boxes, classes, colors, reshape, image, 'cpu')\n",
    "eigencam.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "pytorch-grad-cam GitHub tutorial for object detection with FasterRCNN - https://jacobgil.github.io/pytorch-gradcam-book/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html\n",
    "\n",
    "pytorch-grad-cam GitHub tutorial for object detection with YOLO - https://jacobgil.github.io/pytorch-gradcam-book/EigenCAM%20for%20YOLO5.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
