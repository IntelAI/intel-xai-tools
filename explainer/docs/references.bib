---
@article{bennetot2021practical,
  title={A Practical Tutorial on Explainable AI Techniques},
  author={Bennetot, Adrien and Donadello, Ivan and Qadi, Ayoub El and Dragoni, Mauro and Frossard, Thomas and Wagner, Benedikt and Saranti, Anna and Tulli, Silvia and Trocan, Maria and Chatila, Raja and others},
  journal={arXiv preprint arXiv:2111.14260},
  year={2021}
}
@misc{logictensornetworks,
    title={Logic Tensor Networks (LTN)},
    author={Logic Tensor Networks (LTN)},
    booktitle={GitHub},
    year={2022}
}
@inproceedings{mothilal2020explaining,
  title={Explaining machine learning classifiers through diverse counterfactual explanations},
  author={Mothilal, Ramaravind K and Sharma, Amit and Tan, Chenhao},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={607--617},
  year={2020}
}
@article{chou2022counterfactuals,
  title={Counterfactuals and causability in explainable artificial intelligence: Theory, algorithms, and applications},
  author={Chou, Yu-Liang and Moreira, Catarina and Bruza, Peter and Ouyang, Chun and Jorge, Joaquim},
  journal={Information Fusion},
  volume={81},
  pages={59--83},
  year={2022},
  publisher={Elsevier}
}
@article{YANG202229,
title = {Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond},
journal = {Information Fusion},
volume = {77},
pages = {29-52},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001597},
author = {Guang Yang and Qinghao Ye and Jun Xia},
keywords = {Explainable AI, Information fusion, Multi-domain information fusion, Weakly supervised learning, Medical image analysis},
abstract = {Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems’ black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.}
}
@article{ZHU202253,
title = {Interpretable learning based Dynamic Graph Convolutional Networks for Alzheimer’s Disease analysis},
journal = {Information Fusion},
volume = {77},
pages = {53-61},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001548},
author = {Yonghua Zhu and Junbo Ma and Changan Yuan and Xiaofeng Zhu},
keywords = {Dynamic graph learning, Graph convolutional networks, Interpretable learning, Alzheimer’s disease diagnosis},
abstract = {Graph Convolutional Networks (GCNs) are widely applied in classification tasks by aggregating the neighborhood information of each sample to output robust node embedding. However, conventional GCN methods do not update the graph during the training process so that their effectiveness is always influenced by the quality of the input graph. Moreover, previous GCN methods lack the interpretability to limit their real applications. In this paper, a novel personalized diagnosis technique is proposed for early Alzheimer’s Disease (AD) diagnosis via coupling interpretable feature learning with dynamic graph learning into the GCN architecture. Specifically, the module of interpretable feature learning selects informative features to provide interpretability for disease diagnosis and abandons redundant features to capture inherent correlation of data points. The module of dynamic graph learning adjusts the neighborhood relationship of every data point to output robust node embedding as well as the correlations of all data points to refine the classifier. The GCN module outputs diagnosis results based on the learned inherent graph structure. All three modules are jointly optimized to perform reliable disease diagnosis at an individual level. Experiments demonstrate that our method outputs competitive diagnosis performance as well as provide interpretability for personalized disease diagnosis.}
}
@article{HOLZINGER202128,
title = {Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI},
journal = {Information Fusion},
volume = {71},
pages = {28-37},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000142},
author = {Andreas Holzinger and Bernd Malle and Anna Saranti and Bastian Pfeifer},
keywords = {Information fusion, Explainable AI, xAI, Graph Neural Networks, Multi-modal causability, Knowledge graphs, Counterfactuals},
abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.}
}
---
