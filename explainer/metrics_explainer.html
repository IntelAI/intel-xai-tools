<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Metrics &mdash; Explainer</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" type="text/css" />
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.9.1/mermaid.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Examples" href="examples/index.html" />
    <link rel="prev" title="Language Models" href="lm_layers_explainer.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Explainable AI Tools
          </a>
              <div class="version">
                0.2.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Explainer</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="explanations.html">Explanations</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="feature_attributions_explainer.html">Feature Attributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lm_layers_explainer.html">Language Models</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Metrics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="examples/index.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="cli.html">CLI Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="bibliography/index.html">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_card_gen/index.html">Model Card Generator</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Explainable AI Tools</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Explainer</a> &raquo;</li>
          <li><a href="explanations.html">Explanations</a> &raquo;</li>
      <li>Metrics</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/explainer/metrics_explainer.md" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="metrics">
<h1>Metrics<a class="headerlink" href="#metrics" title="Permalink to this heading"></a></h1>
<div class="cell tag_remove-input docutils container">
</div>
<div class="mermaid">
            graph LR
A(metrics_explainer) --&gt; B(confusion_matrix)
A --&gt; C(plot)
A --&gt; D(pstats)
click B &quot;/explainer/metrics_explainer.html#metrics_explainer.confusion_matrix&quot; &quot;confusion_matrix&quot;
click C &quot;/explainer/metrics_explainer.html#metrics_explainer.plot&quot; &quot;plot&quot;
click D &quot;/explainer/metrics_explainer.html#metrics_explainer.pstats&quot; &quot;pstats&quot;
        </div><p>Several base metrics are provided for ML/DL classification models. These metrics cover model execution and performance and orient the data scientist to where there is potential for classification bias.</p>
<section id="algorithms">
<h2>Algorithms<a class="headerlink" href="#algorithms" title="Permalink to this heading"></a></h2>
<p>Provided with a classfication model’s predictions and their corresponding ground truths, staple performance metrics can be calculated to determine prediction behaviors in the real world. These functions leverage scikit-learn and plotly (eventually) to calculate and visualize said metrics, respectively.</p>
</section>
<section id="environment">
<h2>Environment<a class="headerlink" href="#environment" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Jupyter Notebooks</p></li>
</ul>
</section>
<section id="id1">
<h2>Metrics<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Performance metrics</p>
<ul>
<li><p>Confusion Matrix</p></li>
<li><p>Performance Plots</p></li>
</ul>
</li>
<li><p>Execution metrics</p>
<ul>
<li><p>Python profiler</p></li>
</ul>
</li>
</ul>
</section>
<section id="toolkits">
<h2>Toolkits<a class="headerlink" href="#toolkits" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Scikit-learn</p></li>
<li><p>Plotly</p></li>
<li><p>Python Profilers</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://github.com/scikit-learn/scikit-learn">Scikit-learn</a><br />
<a class="reference external" href="https://github.com/plotly">Plotly</a><br />
<a class="reference external" href="https://github.com/python/cpython/blob/main/Lib/cProfile.py">Python Profiler</a></p>
</section>
<section id="module-metrics_explainer">
<span id="entry-points"></span><h2>Entry Points<a class="headerlink" href="#module-metrics_explainer" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ConfusionMatrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">groundtruth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#ConfusionMatrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.ConfusionMatrix" title="Permalink to this definition"></a></dt>
<dd><p>Confusion matrix object that holds the accuracy for each classification type in Cij, where
C is the confusion matrix, i is a ground truth label and j is the model’s label prediction.
Confusion matrices are tool to help initially diagnose how well the model performs across
all class labels. Checking these accuracies on unseen test data (real-world data) is a
first step in identifying possible sample bias.</p>
<p>This class supports both binary and multi-class classification. Currently, the accuracies
are normalized across each ground truth (row).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<em>ground</em>) – 2-d array (n_samples, n_classes) of the ground truth, integer one-hot-encoded,
correct target values that correspond to the examples used in testing</p></li>
<li><p><strong>predictions</strong> – 2-d array (n_samples, n_classes) of the one-hot-encoded, predicted probabilities
that align with the ground truth array</p></li>
<li><p><strong>labels</strong> – 1-d array of strings that index the label names to the one-hot encodings</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.y_gt">
<span class="sig-name descname"><span class="pre">y_gt</span></span><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.y_gt" title="Permalink to this definition"></a></dt>
<dd><p>1-d array of integer ground truth labels</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.y_pred">
<span class="sig-name descname"><span class="pre">y_pred</span></span><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.y_pred" title="Permalink to this definition"></a></dt>
<dd><p>1-d array of integer class prediction labels</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.labels">
<span class="sig-name descname"><span class="pre">labels</span></span><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.labels" title="Permalink to this definition"></a></dt>
<dd><p>1-d array of label names (strings) corresponding to the integer label indexes</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.arr">
<span class="sig-name descname"><span class="pre">arr</span></span><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.arr" title="Permalink to this definition"></a></dt>
<dd><p>2-d array of float64s holding the confusion matrix</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.df">
<span class="sig-name descname"><span class="pre">df</span></span><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.df" title="Permalink to this definition"></a></dt>
<dd><p>Pandas DataFrame holding the confusion matrix with the associated label names
for the indexes and column names</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.report">
<span class="sig-name descname"><span class="pre">report</span></span><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.report" title="Permalink to this definition"></a></dt>
<dd><p>A string summary of performances for all classes including precision, recall, f1-score,
and support.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="metrics_explainer.ConfusionMatrix.visualize">
<span class="sig-name descname"><span class="pre">visualize</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#ConfusionMatrix.visualize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.ConfusionMatrix.visualize" title="Permalink to this definition"></a></dt>
<dd><p>Plot the confusion matrix on a heatmap</p>
</dd></dl>

<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="metrics_explainer.Plotter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Plotter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">groundtruth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#Plotter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.Plotter" title="Permalink to this definition"></a></dt>
<dd><p>Plotter object that calculates and holds the necessary values for various plots commonly used in
post-training analysis on test data. In particular, these plots utilize thesholding the predicted
probabilities in order to quantify the skill of the model. They provide behavior of the classification
rates dependent upon these probability thresholds and are useful in diagnosing dataset imbalance issues.</p>
<p>This class accepts both binary and multi-class classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> (<em>ground</em>) – 2-d array (n_samples, n_classes) of the ground truth, integer one-hot-encoded,
correct target values that correspond to the examples used in testing</p></li>
<li><p><strong>predictions</strong> – 2-d array (n_samples, n_classes) of the one-hot-encoded, predicted probabilities
that align with the ground truth array</p></li>
<li><p><strong>labels</strong> – 1-d array of strings that index the label names to the one-hot encodings</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.y_gt">
<span class="sig-name descname"><span class="pre">y_gt</span></span><a class="headerlink" href="#metrics_explainer.Plotter.y_gt" title="Permalink to this definition"></a></dt>
<dd><p>2-d array of one-hot-encoded integer ground truth labels</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.y_pred">
<span class="sig-name descname"><span class="pre">y_pred</span></span><a class="headerlink" href="#metrics_explainer.Plotter.y_pred" title="Permalink to this definition"></a></dt>
<dd><p>2-d array of class predicted probabilities</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.labels">
<span class="sig-name descname"><span class="pre">labels</span></span><a class="headerlink" href="#metrics_explainer.Plotter.labels" title="Permalink to this definition"></a></dt>
<dd><p>1-d array of label names (strings) corresponding to the integer label indexes</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.arr">
<span class="sig-name descname"><span class="pre">arr</span></span><a class="headerlink" href="#metrics_explainer.Plotter.arr" title="Permalink to this definition"></a></dt>
<dd><p>2-d array of float64s holding the confusion matrix</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><a class="headerlink" href="#metrics_explainer.Plotter.precision" title="Permalink to this definition"></a></dt>
<dd><p>Python dict that holds a 1-d array of precisions at every threshold step for each class label
where each key corresponds to the class label index. Used for PR curve</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.recall">
<span class="sig-name descname"><span class="pre">recall</span></span><a class="headerlink" href="#metrics_explainer.Plotter.recall" title="Permalink to this definition"></a></dt>
<dd><p>Python dict that holds a 1-d array of recalls at every threshold step for each class label
where each key corresponds to the class label index. Used for PR curve</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.tpr">
<span class="sig-name descname"><span class="pre">tpr</span></span><a class="headerlink" href="#metrics_explainer.Plotter.tpr" title="Permalink to this definition"></a></dt>
<dd><p>Python dict that holds a 1-d array of true-positive rates at every threshold step for each class label
where each key corresponds to the class label index. Used for PR curve</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.fpr">
<span class="sig-name descname"><span class="pre">fpr</span></span><a class="headerlink" href="#metrics_explainer.Plotter.fpr" title="Permalink to this definition"></a></dt>
<dd><p>Python dict that holds a 1-d array of false-positive rates at every threshold step for each class label
where each key corresponds to the class label index. Used for PR curve</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.pr_curve">
<span class="sig-name descname"><span class="pre">pr_curve</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#Plotter.pr_curve"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.Plotter.pr_curve" title="Permalink to this definition"></a></dt>
<dd><p>plots the precision recall curve for all classes present in labels</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="metrics_explainer.Plotter.roc_curve">
<span class="sig-name descname"><span class="pre">roc_curve</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#Plotter.roc_curve"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.Plotter.roc_curve" title="Permalink to this definition"></a></dt>
<dd><p>plots the receiver-operator characteristics curve for all classes present in labels</p>
</dd></dl>

<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html</a>
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="metrics_explainer.confusion_matrix">
<span class="sig-name descname"><span class="pre">confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">groundtruth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#confusion_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.confusion_matrix" title="Permalink to this definition"></a></dt>
<dd><p>Generates instantiation of a ConfusionMatrix object that holds necessary information
and metrics from the confusion matrix</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>groundtruth</strong> – 2-d array (n_samples, n_classes) of the ground truth, integer one-hot-encoded,
correct target values that correspond to the examples used in testing</p></li>
<li><p><strong>predictions</strong> – 2-d array (n_samples, n_classes) of the one-hot-encoded, predicted probabilities
that align with the groundtruth array</p></li>
<li><p><strong>labels</strong> – 1-d array of strings that index the label names to the one-hot encodings</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ConfusionMatrix object instantiation</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#metrics_explainer.ConfusionMatrix" title="metrics_explainer.ConfusionMatrix">ConfusionMatrix</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">explainer.explainers</span> <span class="kn">import</span> <span class="n">metrics_explainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cm</span> <span class="o">=</span> <span class="n">metrics_explainer</span><span class="p">[</span><span class="s1">&#39;confusionmatrix&#39;</span><span class="p">](</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">label_names</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">df</span><span class="p">)</span>
<span class="go">           cat  dog  horse</span>
<span class="go">cat        0.5  0.5        0.0</span>
<span class="go">dog        0.0  1.0        0.0</span>
<span class="go">horse  0.0  0.0        1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="metrics_explainer.plot">
<span class="sig-name descname"><span class="pre">plot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">groundtruth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predictions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#plot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.plot" title="Permalink to this definition"></a></dt>
<dd><p>Generates instantiation of a Plotter object that holds necessary information
to plot the precision-recall curve and receiver-operator characteristics curve</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>groundtruth</strong> – 2-d array (n_samples, n_classes) of the ground truth, integer one-hot-encoded, correct target values
that correspond to the examples used in testing</p></li>
<li><p><strong>predictions</strong> – 2-d array (n_samples, n_classes) of the one-hot-encoded, predicted probabilities
that align with the groundtruth array</p></li>
<li><p><strong>labels</strong> – 1-d array of strings that index the label names to the one-hot encodings</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Plotter object instantiation</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#metrics_explainer.Plotter" title="metrics_explainer.Plotter">Plotter</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">.002</span><span class="p">,</span> <span class="mf">.09</span><span class="p">,</span> <span class="mf">.89</span><span class="p">],</span> <span class="p">[</span><span class="mf">.01</span><span class="p">,</span> <span class="mf">.7</span><span class="p">,</span> <span class="mf">.29</span><span class="p">],</span> <span class="p">[</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">.67</span><span class="p">,</span> <span class="mf">.03</span><span class="p">],</span> <span class="p">[</span><span class="mf">.55</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">.03</span><span class="p">,</span> <span class="mf">.86</span><span class="p">,</span> <span class="mf">.11</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">explainer.explainers</span> <span class="kn">import</span> <span class="n">metrics_explainer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plotter</span> <span class="o">=</span> <span class="n">metrics_explainer</span><span class="p">[</span><span class="s1">&#39;plot&#39;</span><span class="p">](</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">label_names</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plotter</span><span class="o">.</span><span class="n">recall</span>
<span class="go">{0: array([1. , 1. , 0.5, 0.5, 0.5, 0. ]), 1: array([1. , 1. , 1. , 0.5, 0.5, 0. ]), 2: array([1., 1., 1., 1., 1., 0.])}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="metrics_explainer.pstats">
<span class="sig-name descname"><span class="pre">pstats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">command</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/metrics_explainer.html#pstats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#metrics_explainer.pstats" title="Permalink to this definition"></a></dt>
<dd><p>Executes cProfile.run and saves the results in the PStats class as panda DataFrames.
Two DataFrames are stored - A summary and a report which call PStats.summary and PStats.report respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>command</strong> – command to run</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>this class provides summary and report methods to display the DataFrames</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>PStats</p>
</dd>
</dl>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://docs.python.org/3/library/profile.html">https://docs.python.org/3/library/profile.html</a></p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lm_layers_explainer.html" class="btn btn-neutral float-left" title="Language Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="examples/index.html" class="btn btn-neutral float-right" title="Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>